# -*- coding: utf-8 -*-
"""part_of_speech.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G1DE14GJWdyfIo739McPf-B5fvZ7jZsa
"""

#!pip install -U spacy

#!python -m spacy download en

#!pip install docx2txt

from google.colab import drive
drive.mount('/gdrive')

ls "/gdrive/My Drive/GE_onco"

def mridict(sent):
  a=[]
  print(sent.text)
  for token in sent:
    if token.pos_=='PUNCT' or token.pos_=="SPACE":
      continue
    b=(token.text,token.pos_,token.is_alpha)
    a.append(b)
  print(a)  
  return a

spacy.__version__

from spacy.lang.en import English

def get_sentences(text):

  nlp = English()
  sentencizer = nlp.create_pipe("sentencizer")
  nlp.add_pipe(sentencizer)
  doc = nlp(text)
  return list(doc.sents)

text = docx2txt.process(r"/gdrive/My Drive/GE_onco/MRI.docx")
# text = docx2txt.process(r"/gdrive/My Drive/MRI.docx")
sents = get_sentences(text)

nlp = spacy.load("en_core_web_sm")
for sent in sents:
  res = nlp(sent.text)
  print(res.text)
  print(list(res.noun_chunks))
  print(res.sentiment)
#   for chunk in sent.noun_chunks:
#     print(chunk)

import spacy
# import docx2txt, json, collections
text = docx2txt.process(r"/gdrive/My Drive/GE_onco/MRI.docx") 
nlp = spacy.load("en_core_web_sm")
doc =  nlp(text)
dict1={}
i=0
for sent in doc.sents:
  dict1[i]=mridict(sent)
  i=i+1
  if i > 20:
    break
  
# with open('output.json','w') as json_file:
#     json.dump(dict1,json_file)

